import json
import os
from typing import TypedDict, Annotated, Literal
from langgraph.graph import StateGraph, END
from schema_linking_generation import SchemaLinkingNode
from sql_generation import SQLGenerationNode
from sql_checker import SQLCheckerNode
from sql_exe import execute_sql


# å®šä¹‰ LangGraph çŠ¶æ€
class GraphState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    # è¾“å…¥å­—æ®µ
    sql_id: str
    question: str
    table_list: list
    knowledge: str
    å¤æ‚åº¦: str

    # ä¸­é—´ç»“æœ
    schema_links: list  # Schema Linking ç»“æœ
    table_schemas: str
    sql: str  # ç”Ÿæˆçš„ SQL

    # æ§åˆ¶å­—æ®µ
    retry_count: int  # é‡è¯•æ¬¡æ•°
    max_retries: int  # æœ€å¤§é‡è¯•æ¬¡æ•°
    is_valid: bool  # SQL æ˜¯å¦æœ‰æ•ˆ
    error_message: str  # é”™è¯¯æ¶ˆæ¯


# èŠ‚ç‚¹1: Schema Linking
def schema_linking_node(state: GraphState) -> GraphState:
    """Schema Linking èŠ‚ç‚¹ - è¯†åˆ«éœ€è¦çš„è¡¨ã€åˆ—å’Œå€¼"""
    print("\n" + "=" * 80)
    print(f"æ­¥éª¤1: Schema Linking (å°è¯• {state['retry_count'] + 1}/{state['max_retries']})")
    print("=" * 80)

    schema_node = SchemaLinkingNode()
    schema_links = schema_node.run({
        'question': state['question'],
        'table_list': state['table_list'],
        'knowledge': state['knowledge']
    })

    print(f"\nè¯†åˆ«åˆ° {len(schema_links)} ä¸ª schema links")
    for i, link in enumerate(schema_links[:5], 1):
        print(f"  {i}. {link}")
    if len(schema_links) > 5:
        print(f"  ... è¿˜æœ‰ {len(schema_links) - 5} ä¸ª")

    return {
        **state,
        'schema_links': schema_links
    }


# èŠ‚ç‚¹2: SQL Generation
def sql_generation_node(state: GraphState) -> GraphState:
    """SQL Generation èŠ‚ç‚¹ - åŸºäº Schema Links ç”Ÿæˆ SQL"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤2: SQL Generation")
    print("=" * 80)

    sql_node = SQLGenerationNode()
    result = sql_node.run({
        'question': state['question'],
        'table_list': state['table_list'],
        'knowledge': state['knowledge'],
        'schema_links': state['schema_links']
    })

    generated_sql = result.get('sql', '')

    print("\nç”Ÿæˆçš„ SQL:")
    print("-" * 80)
    print(generated_sql[:500] + "..." if len(generated_sql) > 500 else generated_sql)
    print("-" * 80)

    return {
        **state,
        'sql': generated_sql
    }


# èŠ‚ç‚¹3: SQL Checker
def sql_checker_node(state: GraphState) -> GraphState:
    """SQL Checker èŠ‚ç‚¹ - æ£€æŸ¥ SQL æ­£ç¡®æ€§"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤3: SQL Checker")
    print("=" * 80)

    checker_node = SQLCheckerNode()
    result = checker_node.run({
        'sql': state['sql']
    })

    is_valid = result.get('is_valid', False)
    message = result.get('message', '')

    if is_valid:
        print(f"\nâœ… SQL æ£€æŸ¥é€šè¿‡ï¼")
    else:
        print(f"\nâŒ SQL æ£€æŸ¥å¤±è´¥: {message}")
        print(f"å½“å‰é‡è¯•æ¬¡æ•°: {state['retry_count'] + 1}/{state['max_retries']}")

    return {
        **state,
        'is_valid': is_valid,
        'error_message': message,
        'retry_count': state['retry_count'] + 1
    }


# æ¡ä»¶è¾¹: å†³å®šæ˜¯ç»§ç»­è¿˜æ˜¯é‡è¯•
def should_retry(state: GraphState) -> Literal["end", "retry"]:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•

    Returns:
        "end": SQL æ­£ç¡®æˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»“æŸæµç¨‹
        "retry": SQL é”™è¯¯ä¸”æœªè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå›åˆ° Schema Linking
    """
    if state['is_valid']:
        print("\nâœ… SQL éªŒè¯é€šè¿‡ï¼Œæµç¨‹ç»“æŸ")
        return "end"

    if state['retry_count'] >= state['max_retries']:
        print(f"\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({state['max_retries']})ï¼Œæµç¨‹ç»“æŸ")
        return "end"

    print(f"\nğŸ”„ å‡†å¤‡é‡è¯• ({state['retry_count']}/{state['max_retries']})...")
    return "retry"


# æ„å»º LangGraph å·¥ä½œæµ
def build_workflow() -> StateGraph:
    """æ„å»º LangGraph å·¥ä½œæµ"""
    workflow = StateGraph(GraphState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("schema_linking", schema_linking_node)
    workflow.add_node("sql_generation", sql_generation_node)
    workflow.add_node("sql_checker", sql_checker_node)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("schema_linking")

    # æ·»åŠ è¾¹
    workflow.add_edge("schema_linking", "sql_generation")
    workflow.add_edge("sql_generation", "sql_checker")

    # æ·»åŠ æ¡ä»¶è¾¹ï¼ˆæ ¹æ® SQL æ£€æŸ¥ç»“æœå†³å®šæ˜¯ç»“æŸè¿˜æ˜¯é‡è¯•ï¼‰
    workflow.add_conditional_edges(
        "sql_checker",
        should_retry,
        {
            "end": END,
            "retry": "schema_linking"  # é‡è¯•æ—¶å›åˆ° Schema Linking
        }
    )

    return workflow.compile()


def single_pipeline(input_dict):
    """
    ä¸ºä¸€æ¡æ•°æ®ç”ŸæˆSQLæŸ¥è¯¢è¯­å¥ï¼ˆä½¿ç”¨ LangGraphï¼‰

    Args:
        input_dict: è¾“å…¥å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - sql_id: SQL ID
            - question: è‡ªç„¶è¯­è¨€é—®é¢˜
            - sql: å‚è€ƒSQLï¼ˆå¯é€‰ï¼‰
            - å¤æ‚åº¦: é—®é¢˜å¤æ‚åº¦
            - table_list: æ¶‰åŠçš„è¡¨åˆ—è¡¨
            - knowledge: ç›¸å…³çŸ¥è¯†

    Returns:
        dict: è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - sql_id: SQL ID
            - sql: ç”Ÿæˆçš„SQL
            - retry_count: é‡è¯•æ¬¡æ•°
            - is_valid: SQL æ˜¯å¦é€šè¿‡éªŒè¯
    """
    print("\n" + "â–ˆ" * 80)
    print(f"å¼€å§‹å¤„ç†: {input_dict['sql_id']}")
    print(f"é—®é¢˜: {input_dict['question'][:100]}...")
    print("â–ˆ" * 80)

    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        'sql_id': input_dict['sql_id'],
        'question': input_dict['question'],
        'table_list': input_dict['table_list'],
        'knowledge': input_dict.get('knowledge', ''),
        'å¤æ‚åº¦': input_dict.get('å¤æ‚åº¦', ''),
        'schema_links': [],
        'sql': '',
        'retry_count': 0,
        'max_retries': 3,  # æœ€å¤šé‡è¯•3æ¬¡
        'is_valid': False,
        'error_message': ''
    }

    # æ„å»ºå¹¶è¿è¡Œå·¥ä½œæµ
    app = build_workflow()
    final_state = app.invoke(initial_state)

    # è¾“å‡ºç»“æœ
    print("\n" + "â–ˆ" * 80)
    print("å¤„ç†å®Œæˆ")
    print("â–ˆ" * 80)
    print(f"SQL ID: {final_state['sql_id']}")
    print(f"é‡è¯•æ¬¡æ•°: {final_state['retry_count']}")
    print(f"éªŒè¯ç»“æœ: {'âœ… é€šè¿‡' if final_state['is_valid'] else 'âŒ å¤±è´¥'}")
    print("\næœ€ç»ˆ SQL:")
    print("-" * 80)
    print(final_state['sql'])
    print("-" * 80)

    return {
        'sql_id': final_state['sql_id'],
        'sql': final_state['sql'],
        'is_valid': final_state['is_valid']
    }


def batch_pipeline(input_file: str, output_file: str):
    """
    æ‰¹é‡å¤„ç†æ•°æ®é›†

    Args:
        input_file: è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ false.jsonï¼‰
        output_file: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    print(f"\nåŠ è½½äº† {len(dataset)} æ¡æ•°æ®")

    results = []
    for i, item in enumerate(dataset, 1):
        print(f"\n{'='*80}")
        print(f"å¤„ç†è¿›åº¦: {i}/{len(dataset)}")
        print(f"{'='*80}")

        result = single_pipeline(item)
        results.append(result)
    # ä¿å­˜ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*80}")
    print(f"å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"{'='*80}")

    # ç»Ÿè®¡ä¿¡æ¯
    valid_count = sum(1 for r in results if r.get('is_valid', False))
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ•°: {len(results)}")
    print(f"  éªŒè¯é€šè¿‡: {valid_count}")
    print(f"  éªŒè¯å¤±è´¥: {len(results) - valid_count}")
    print(f"  é€šè¿‡ç‡: {valid_count / len(results) * 100:.2f}%")


if __name__ == "__main__":
    # æµ‹è¯•å•æ¡æ•°æ®
    test_data = {
        "sql_id": "test_001",
        "question": "ç»Ÿè®¡2024å¹´1æœˆåˆ°3æœˆæ¯æœˆçš„æ´»è·ƒç”¨æˆ·æ•°",
        "å¤æ‚åº¦": "ç®€å•",
        "table_list": ["dws_mgamejp_login_user_activity_di"],
        "knowledge": ""
    }

    print("=" * 80)
    print("LangGraph SQL ç”Ÿæˆæµç¨‹æµ‹è¯•")
    print("=" * 80)

    # è¿è¡Œæµ‹è¯•
    result = single_pipeline(test_data)

    print("\n" + "=" * 80)
    print("æœ€ç»ˆç»“æœ:")
    print("=" * 80)
    print(json.dumps(result, ensure_ascii=False, indent=2))

    # æ‰¹é‡å¤„ç†ç¤ºä¾‹ï¼ˆæ³¨é‡Šæ‰ï¼Œéœ€è¦æ—¶å–æ¶ˆæ³¨é‡Šï¼‰
    # batch_pipeline(
    #     input_file='data/false.json',
    #     output_file='output/generated_sqls.json'
    # )
