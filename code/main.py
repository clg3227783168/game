import json
import os
from typing import TypedDict, Annotated, Literal
from langgraph.graph import StateGraph, END
from schema_linking_generation import SchemaLinkingNode
from sql_generation import SQLGenerationNode
from sql_checker import SQLCheckerNode

# å®šä¹‰ LangGraph çŠ¶æ€
class GraphState(TypedDict):
    """å·¥ä½œæµçŠ¶æ€å®šä¹‰"""
    # è¾“å…¥å­—æ®µ
    sql_id: str
    question: str
    table_list: list
    knowledge: str
    å¤æ‚åº¦: str

    # ä¸­é—´ç»“æœ
    schema_links: list  # Schema Linking ç»“æœ
    table_schemas: str
    sql: str  # ç”Ÿæˆçš„ SQL

    # æ§åˆ¶å­—æ®µ
    retry_count: int  # é‡è¯•æ¬¡æ•°
    max_retries: int  # æœ€å¤§é‡è¯•æ¬¡æ•°
    is_valid: bool  # SQL æ˜¯å¦æœ‰æ•ˆ
    error_message: str  # é”™è¯¯æ¶ˆæ¯


# èŠ‚ç‚¹1: Schema Linking
def schema_linking_node(state: GraphState) -> GraphState:
    """Schema Linking èŠ‚ç‚¹ - è¯†åˆ«éœ€è¦çš„è¡¨ã€åˆ—å’Œå€¼"""

    schema_node = SchemaLinkingNode()
    output = schema_node.run({
        'question': state['question'],
        'table_list': state['table_list'],
        'knowledge': state['knowledge']
    })
    return {
        **state,
        'schema_links': output['schema_links'],
        'table_schemas': output['table_schemas']
    }


# èŠ‚ç‚¹2: SQL Generation
def sql_generation_node(state: GraphState) -> GraphState:

    sql_node = SQLGenerationNode()
    result = sql_node.run({
        'sql_id': state['sql_id'],
        'question': state['question'],
        'table_list': state['table_list'],
        'knowledge': state['knowledge'],
        'schema_links': state['schema_links'],
        'table_schemas': state['table_schemas']
    })

    generated_sql = result.get('sql', '')

    print("\nç”Ÿæˆçš„ SQL:")
    print("-" * 80)
    print(generated_sql[:500] + "..." if len(generated_sql) > 500 else generated_sql)
    print("-" * 80)

    return {
        **state,
        'sql': generated_sql
    }


# èŠ‚ç‚¹3: SQL Checker
def sql_checker_node(state: GraphState) -> GraphState:
    """SQL Checker èŠ‚ç‚¹ - æ£€æŸ¥ SQL æ­£ç¡®æ€§"""
    print("\n" + "=" * 80)
    print("æ­¥éª¤3: SQL Checker")
    print("=" * 80)

    checker_node = SQLCheckerNode()
    result = checker_node.run({
        'sql': state['sql']
    })

    is_valid = result.get('is_valid', False)
    message = result.get('message', '')

    if is_valid:
        print(f"\nâœ… SQL æ£€æŸ¥é€šè¿‡ï¼")
    else:
        print(f"\nâŒ SQL æ£€æŸ¥å¤±è´¥: {message}")
        print(f"å½“å‰é‡è¯•æ¬¡æ•°: {state['retry_count'] + 1}/{state['max_retries']}")

    return {
        **state,
        'is_valid': is_valid,
        'error_message': message,
        'retry_count': state['retry_count'] + 1
    }


# æ¡ä»¶è¾¹: å†³å®šæ˜¯ç»§ç»­è¿˜æ˜¯é‡è¯•
def should_retry(state: GraphState) -> Literal["end", "retry"]:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•

    Returns:
        "end": SQL æ­£ç¡®æˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»“æŸæµç¨‹
        "retry": SQL é”™è¯¯ä¸”æœªè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå›åˆ° Schema Linking
    """
    if state['is_valid']:
        print("\nâœ… SQL éªŒè¯é€šè¿‡ï¼Œæµç¨‹ç»“æŸ")
        return "end"

    if state['retry_count'] >= state['max_retries']:
        print(f"\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({state['max_retries']})ï¼Œæµç¨‹ç»“æŸ")
        return "end"

    print(f"\nğŸ”„ å‡†å¤‡é‡è¯• ({state['retry_count']}/{state['max_retries']})...")
    return "retry"


# æ„å»º LangGraph å·¥ä½œæµ
def build_workflow() -> StateGraph:
    """æ„å»º LangGraph å·¥ä½œæµ"""
    workflow = StateGraph(GraphState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("schema_linking", schema_linking_node)
    workflow.add_node("sql_generation", sql_generation_node)
    workflow.add_node("sql_checker", sql_checker_node)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("schema_linking")

    # æ·»åŠ è¾¹
    workflow.add_edge("schema_linking", "sql_generation")
    workflow.add_edge("sql_generation", "sql_checker")

    # æ·»åŠ æ¡ä»¶è¾¹ï¼ˆæ ¹æ® SQL æ£€æŸ¥ç»“æœå†³å®šæ˜¯ç»“æŸè¿˜æ˜¯é‡è¯•ï¼‰
    workflow.add_conditional_edges(
        "sql_checker",
        should_retry,
        {
            "end": END,
            "retry": "schema_linking"  # é‡è¯•æ—¶å›åˆ° Schema Linking
        }
    )

    return workflow.compile()


def single_pipeline(input_dict):
    """
    ä¸ºä¸€æ¡æ•°æ®ç”ŸæˆSQLæŸ¥è¯¢è¯­å¥ï¼ˆä½¿ç”¨ LangGraphï¼‰

    Args:
        input_dict: è¾“å…¥å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - sql_id: SQL ID
            - question: è‡ªç„¶è¯­è¨€é—®é¢˜
            - sql: å‚è€ƒSQLï¼ˆå¯é€‰ï¼‰
            - å¤æ‚åº¦: é—®é¢˜å¤æ‚åº¦
            - table_list: æ¶‰åŠçš„è¡¨åˆ—è¡¨
            - knowledge: ç›¸å…³çŸ¥è¯†

    Returns:
        dict: è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - sql_id: SQL ID
            - sql: ç”Ÿæˆçš„SQL
            - retry_count: é‡è¯•æ¬¡æ•°
            - is_valid: SQL æ˜¯å¦é€šè¿‡éªŒè¯
    """
    print("\n" + "â–ˆ" * 80)
    print(f"å¼€å§‹å¤„ç†: {input_dict['sql_id']}")
    print(f"é—®é¢˜: {input_dict['question'][:100]}...")
    print("â–ˆ" * 80)

    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        'sql_id': input_dict['sql_id'],
        'question': input_dict['question'],
        'table_list': input_dict['table_list'],
        'knowledge': input_dict.get('knowledge', ''),
        'å¤æ‚åº¦': input_dict.get('å¤æ‚åº¦', ''),
        'schema_links': [],
        'sql': '',
        'retry_count': 0,
        'max_retries': 3,  # æœ€å¤šé‡è¯•3æ¬¡
        'is_valid': False,
        'error_message': ''
    }

    # æ„å»ºå¹¶è¿è¡Œå·¥ä½œæµ
    app = build_workflow()
    final_state = app.invoke(initial_state)

    # è¾“å‡ºç»“æœ
    print("\n" + "â–ˆ" * 80)
    print("å¤„ç†å®Œæˆ")
    print("â–ˆ" * 80)
    print(f"SQL ID: {final_state['sql_id']}")
    print(f"é‡è¯•æ¬¡æ•°: {final_state['retry_count']}")
    print(f"éªŒè¯ç»“æœ: {'âœ… é€šè¿‡' if final_state['is_valid'] else 'âŒ å¤±è´¥'}")
    print("\næœ€ç»ˆ SQL:")
    print("-" * 80)
    print(final_state['sql'])
    print("-" * 80)

    return {
        'sql_id': final_state['sql_id'],
        'sql': final_state['sql'],
        'is_valid': final_state['is_valid']
    }


def batch_pipeline(input_file: str, output_file: str):
    """
    æ‰¹é‡å¤„ç†æ•°æ®é›†

    Args:
        input_file: è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ false.jsonï¼‰
        output_file: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½è¾“å…¥æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    print(f"\nåŠ è½½äº† {len(dataset)} æ¡æ•°æ®")

    # åŠ è½½å·²æœ‰ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    existing_results = {}
    if os.path.exists(output_file):
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
                # å°†å·²æœ‰ç»“æœè½¬æ¢ä¸ºå­—å…¸ï¼Œæ–¹ä¾¿æŸ¥è¯¢
                existing_results = {r['sql_id']: r for r in existing_data}
                print(f"å‘ç°å·²æœ‰ç»“æœæ–‡ä»¶ï¼ŒåŒ…å« {len(existing_results)} æ¡è®°å½•")
        except Exception as e:
            print(f"è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            existing_results = {}

    # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„æ•°æ®ï¼ˆæ’é™¤å·²å­˜åœ¨çš„ sql_idï¼‰
    todo_items = []
    skipped_count = 0
    for item in dataset:
        sql_id = item.get('sql_id')
        if sql_id in existing_results:
            skipped_count += 1
            print(f"è·³è¿‡å·²å­˜åœ¨çš„è®°å½•: {sql_id}")
        else:
            todo_items.append(item)

    print(f"\néœ€è¦å¤„ç†: {len(todo_items)} æ¡æ•°æ®")
    print(f"è·³è¿‡å·²å­˜åœ¨: {skipped_count} æ¡æ•°æ®")

    # å¤„ç†æ–°æ•°æ®
    new_results = []
    for i, item in enumerate(todo_items, 1):
        print(f"\n{'='*80}")
        print(f"å¤„ç†è¿›åº¦: {i}/{len(todo_items)}")
        print(f"{'='*80}")

        result = single_pipeline(item)
        new_results.append(result)
        existing_results[result['sql_id']] = result

        # æ¯å¤„ç†ä¸€æ¡å°±ä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢ä¸­æ–­å¯¼è‡´æ•°æ®ä¸¢å¤±
        all_results = list(existing_results.values())
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)

    # æœ€ç»ˆä¿å­˜
    all_results = list(existing_results.values())
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*80}")
    print(f"å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"{'='*80}")

    # ç»Ÿè®¡ä¿¡æ¯
    valid_count = sum(1 for r in all_results if r.get('is_valid', False))
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ•°: {len(all_results)}")
    print(f"  æœ¬æ¬¡æ–°å¢: {len(new_results)}")
    print(f"  éªŒè¯é€šè¿‡: {valid_count}")
    print(f"  éªŒè¯å¤±è´¥: {len(all_results) - valid_count}")


if __name__ == "__main__":
    # æµ‹è¯•å•æ¡æ•°æ®
    # test_data = {
    #     "sql_id": "sql_1",
    #     "question": "ç»Ÿè®¡2025.07.24çš„æ‰‹æ¸¸å…¨é‡ç”¨æˆ·ä¸”æ ‡ç­¾ä¸ºå…¶ä»–ï¼Œåœ¨ç«å“ä¸šåŠ¡ä¸‹2025.05.30-2025.07.24çš„åœ¨çº¿æ—¶é•¿ã€‚\nè¾“å‡ºï¼šsuseridã€sgamecodeã€ionlinetime\n\n",
    #     "å¤æ‚åº¦": "ä¸­ç­‰",
    #     "table_list": [
    #         "dws_mgamejp_login_user_activity_di",
    #         "dim_vplayerid_vies_df"
    #     ],
    #     "knowledge": "ç«å“ä¸šåŠ¡ï¼š\nsgamecode in (\"initiatived\",\"jordass\",\"esports\",\"allianceforce\",\"strategy\",\"playzone\",\"su\")\nsaccounttype = \"-100\" -- è´¦å·ä½“ç³»ï¼Œå–-100è¡¨ç¤ºæ±‡æ€»\nand suseridtype in (\"qq\",\"wxid\") -- ç”¨æˆ·ç±»å‹\nand splattype = \"-100\" -- å¹³å°ç±»å‹\nand splat = \"-100\" -- å¹³å°ï¼Œå†™æ­»ä¸º-100\n"
    # }

    # print("=" * 80)
    # print("LangGraph SQL ç”Ÿæˆæµç¨‹æµ‹è¯•")
    # print("=" * 80)

    # è¿è¡Œæµ‹è¯•
    # result = single_pipeline(test_data)

    # print("\n" + "=" * 80)
    # print("æœ€ç»ˆç»“æœ:")
    # print("=" * 80)
    # print(json.dumps(result, ensure_ascii=False, indent=2))

    # æ‰¹é‡å¤„ç†ç¤ºä¾‹ï¼ˆæ³¨é‡Šæ‰ï¼Œéœ€è¦æ—¶å–æ¶ˆæ³¨é‡Šï¼‰
    batch_pipeline(
        input_file='code/data/false.json',
        output_file='code/data/generated_sqls.json'
    )
